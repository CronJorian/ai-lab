{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointNet Transfer Learning on subset of ModelNet10\n",
    "\n",
    "\n",
    "The ModelNet10 dataset contains 4899 3D Meshes from 10 different classes.\n",
    "\n",
    "The classes are:\n",
    "\n",
    "* bed\n",
    "* monitor\n",
    "* desk\n",
    "* chair\n",
    "* dresser\n",
    "* toilet\n",
    "* sofa\n",
    "* table\n",
    "* night stand\n",
    "* bathttub \n",
    "\n",
    "We provide you a pretrained version of PointNet. It was trained on 6 of the above 10 classes. Your task is to implement the PointNet Model and train it to classify the other 4 classes using transfer learning.\n",
    "\n",
    "The classes to be learnt are:\n",
    "\n",
    "* desk\n",
    "* chair\n",
    "* toilet \n",
    "* table\n",
    "\n",
    "\n",
    "\n",
    "Tasks:\n",
    "* implement the TODOs\n",
    "* transfer learn the PointNet to achieve an Accuracy > 90%\n",
    "Help:\n",
    "* use the PointNet Paper [clickedy](http://stanford.edu/~rqi/pointnet/)\n",
    "* use the Keras API Documentation [clickedy](https://www.keras.io/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install open3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "%load_ext autoreload\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# download ModelNet10\n",
    "wget -q http://3dvision.princeton.edu/projects/2014/3DShapeNets/ModelNet10.zip\n",
    "# unpack\n",
    "unzip -oq ModelNet10.zip\n",
    "# remove Archive\n",
    "rm ModelNet10.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP = [\"chair\", \"desk\", \"toilet\", \"table\"]\n",
    "classes = [f for f in os.scandir(\"ModelNet10/\") if os.path.isdir(f)]\n",
    "\n",
    "import shutil\n",
    "\n",
    "for c in classes:\n",
    "    if c.name not in KEEP:\n",
    "        shutil.rmtree(c.path, ignore_errors=False, onerror=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part One: Data Provider\n",
    "\n",
    "We use the Keras Sequence API to construct a data provider, which feeds our model during training and validation. Fill in all the ToDos to make it work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelNetProvider(Sequence):\n",
    "    \"\"\"\n",
    "    Lazily load point clouds and annotations from filesystem and prepare it for model training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset, batch_size, n_classes, sample_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        # indices of samples used for shuffling\n",
    "        self.indices = # ToDo\n",
    "\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return  # ToDo: Define the length of the generator (Hint: Number of Steps in one Epoch)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"Generate one batch of data.\"\"\"\n",
    "        batch_indices = # ToDo: select the indices for the current batch, starting at `index`\n",
    "        batch_samples = self.dataset.iloc[batch_indices]\n",
    "\n",
    "        return self.__generate_data(batch_samples)\n",
    "\n",
    "    def __generate_data(self, batch_samples):\n",
    "        X = []\n",
    "        y = []\n",
    "        for i, row in batch_samples.iterrows():\n",
    "            mesh = o3d.io.read_triangle_mesh((row[\"path\"]))\n",
    "            pcd = mesh.sample_points_uniformly(number_of_points=self.sample_size)\n",
    "            points = np.asarray(pcd.points)\n",
    "            centered_points = # ToDo center the points to origin\n",
    "            normalized_points = # ToDo normalize the centered points\n",
    "            X.append(normalized_points)\n",
    "            y.append(row[\"class\"])\n",
    "\n",
    "        return self.rotate_point_clouds(np.array(X)), to_categorical(np.array(y), num_classes=self.n_classes)\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle training data, so batches are in different order\"\"\"\n",
    "        np.random.shuffle(self.indices)\n",
    "        \n",
    "        \n",
    "    def rotate_point_clouds(self, batch, rotation_angle_range=(-np.pi/8, np.pi/8)):\n",
    "        \"\"\"Rotate point cloud around y-axis (=up) by random angle\"\"\"\n",
    "        for b, pc in enumerate(batch):\n",
    "            phi = np.random.uniform(*rotation_angle_range)\n",
    "            c, s = np.cos(phi), np.sin(phi)\n",
    "            R = np.matrix([[c, 0, s],\n",
    "                           [0, 1, 0],\n",
    "                           [-s, 0, c]])\n",
    "            batch[b, :, :3] = np.dot(pc[:, :3], R)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataset(data_directory, file_extension=\".off\"):\n",
    "    \"\"\"\n",
    "    Loads an index to all files and structures them.\n",
    "    :param data_directory: directory containing the data files\n",
    "    :param file_extension: extension of the data files\n",
    "    :return: pandas dataframe containing an index to all files and a label index, \n",
    "        mapping numerical label representations to label names.\n",
    "    \"\"\"\n",
    "    files = [\n",
    "        os.path.join(r, f)  \n",
    "            for r, d, fs in os.walk(data_directory) \n",
    "            for f in fs if f.endswith(file_extension)\n",
    "        ]\n",
    "    \n",
    "    dataframe = pd.DataFrame({\n",
    "        \"path\": files,\n",
    "        \"class\": pd.Categorical([f.rsplit(\"/\", 3)[1] for f in files]),\n",
    "        \"is_train\": [\"train\" in f for f in files]\n",
    "    })\n",
    "    \n",
    "    factorization = dataframe[\"class\"].factorize()\n",
    "    dataframe[\"class\"] = factorization[0]\n",
    "    \n",
    "    return dataframe, factorization[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Two: PointNet Architecture\n",
    "\n",
    "Implement all the missing code pieces to complete the whole architecture. Follow the description in the paper or use the image below. If your model definition is right, you should be able to load our pretrained weigths without an error. \n",
    "\n",
    "We provided the definition of the T-Net, so you don't need to implement it on your own. If you are interested in the implementation details, just have a look at the pointnet_utils.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![PointNet Architecture](http://stanford.edu/~rqi/pointnet/images/pointnet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Dropout, Dense, Dot, Lambda, \\\n",
    "    Reshape, concatenate, GlobalMaxPooling1D, BatchNormalization, \\\n",
    "    Activation, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.initializers import Constant\n",
    "from tensorflow.keras.regularizers import Regularizer\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions for convolutions and densely connected layers with batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1d_bn(x, num_filters, kernel_size, padding='same', strides=1,\n",
    "              use_bias=False, scope=None, activation='relu'):\n",
    "    \"\"\"\n",
    "    Utility function to apply Convolution + Batch Normalization.\n",
    "    \"\"\"\n",
    "    with K.name_scope(scope):\n",
    "        input_shape = x.get_shape().as_list()[-2:]\n",
    "        x = # ToDo define the Convolutional Part of this Layer\n",
    "        x = # ToDo add BatchNormalization\n",
    "        x = # ToDo add activation function\n",
    "    return x\n",
    "\n",
    "\n",
    "def dense_bn(x, units, use_bias=True, scope=None, activation=None):\n",
    "    \"\"\"\n",
    "    Utility function to apply Dense + Batch Normalization.\n",
    "    \"\"\"\n",
    "    with K.name_scope(scope):\n",
    "        x = # ToDo Add Dense Part of this Layer\n",
    "        x = # ToDo add BatchNormalization\n",
    "        x = # ToDo add activation function\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Own Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrthogonalRegularizer(Regularizer):\n",
    "    \"\"\"\n",
    "    Considering that input is flattened square matrix X, regularizer tries to ensure that matrix X\n",
    "    is orthogonal, i.e. ||X*X^T - I|| = 0. L1 and L2 penalties can be applied to it\n",
    "    \"\"\"    \n",
    "    def __init__(self, l1=0.0, l2=0.0):\n",
    "        self.l1 = K.cast_to_floatx(l1)\n",
    "        self.l2 = K.cast_to_floatx(l2)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        size = int(np.sqrt(x.shape[1].value))\n",
    "        assert size * size == x.shape[1].value\n",
    "        x = K.reshape(x, (-1, size, size))\n",
    "        xxt = # ToDo compute dot product on the batches\n",
    "        regularization = 0.0\n",
    "        if self.l1:\n",
    "            regularization += # ToDo: Implement L1 regularization\n",
    "        if self.l2:\n",
    "            regularization += # ToDo: Implement L2 regularization\n",
    "\n",
    "        return regularization\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {'l1': float(self.l1), 'l2': float(self.l2)}\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "def orthogonal(l1=0.0, l2=0.0):\n",
    "    \"\"\"\n",
    "    Functional wrapper for OrthogonalRegularizer.\n",
    "    :param l1: l1 penalty\n",
    "    :param l2: l2 penalty\n",
    "    :return: Orthogonal regularizer to append to a loss function\n",
    "    \"\"\"\n",
    "    return OrthogonalRegularizer(l1, l2)\n",
    "\n",
    "\n",
    "def transform_net(inputs, scope=None, regularize=False):\n",
    "    \"\"\"\n",
    "    Generates an orthogonal transformation tensor for the input preprocessing\n",
    "    :param inputs: tensor with input image (either BxNxK or BxNx1xK)\n",
    "    :param scope: name of the grouping scope\n",
    "    :param regularize: enforce orthogonality constraint\n",
    "    :return: BxKxK tensor of the transformation\n",
    "    \"\"\"\n",
    "    with K.name_scope(scope):\n",
    "\n",
    "        input_shape = inputs.get_shape().as_list()\n",
    "        k = input_shape[-1]\n",
    "\n",
    "        net = conv1d_bn(inputs, num_filters=64, kernel_size=1, padding='valid',\n",
    "                        use_bias=True, scope='tconv1')\n",
    "        net = conv1d_bn(net, num_filters=128, kernel_size=1, padding='valid',\n",
    "                        use_bias=True, scope='tconv2')\n",
    "        net = conv1d_bn(net, num_filters=1024, kernel_size=1, padding='valid',\n",
    "                        use_bias=True, scope='tconv3')\n",
    "\n",
    "        net = GlobalMaxPooling1D(data_format='channels_last')(net)\n",
    "\n",
    "        net = dense_bn(net, units=512, scope='tfc1', activation='relu')\n",
    "        net = dense_bn(net, units=256, scope='tfc2', activation='relu')\n",
    "\n",
    "        transform = Dense(units=k * k,\n",
    "                          kernel_initializer='zeros', bias_initializer=Constant(np.eye(k).flatten()),\n",
    "                          activity_regularizer=orthogonal(l2=0.001) if regularize else None)(net)\n",
    "\n",
    "        transform = Reshape((k, k))(transform)\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PointNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointnet(input_shape, classes):\n",
    "    \"\"\"\n",
    "    PointNet Model definition for classification.\n",
    "    :param input_shape: The point cloud shape\n",
    "    :param classes: Number of classes in output.\n",
    "    :return: PointNet Model for classification with `classes` classes.\n",
    "    \"\"\"\n",
    "    # Generate input tensor\n",
    "    inputs = # ToDo define Input-Layer\n",
    "\n",
    "    # Obtain spatial point transform from inputs and convert inputs\n",
    "    ptransform = transform_net(inputs, scope='transform_net1', regularize=False)\n",
    "    point_cloud_transformed = # ToDo Perform the matrix multiply between ptransform and inputs\n",
    "\n",
    "    # First block of convolutions\n",
    "    net = # ToDo, define first Conv-Layer\n",
    "    net = # ToDo, define second Conv-Layer\n",
    "\n",
    "    # Obtain feature transform and apply it to the network\n",
    "    ftransform = transform_net(net, scope='transform_net2', regularize=True)\n",
    "    net_transformed = # ToDo Perform the matrix multiply between ftransform and net\n",
    "\n",
    "    # Second block of convolutions\n",
    "    net = # ToDo, define third Conv-Layer\n",
    "    net = # ToDo, define fourth Conv-Layer\n",
    "    net = # ToDo, define fifth Conv-Layer\n",
    "\n",
    "    # add Maxpooling \n",
    "    net = # Todo define MaxPool Layer\n",
    "\n",
    "    # Top layers\n",
    "    net = # ToDo, define first Dense-Layer\n",
    "    net = # ToDo, define first Dropout-Layer\n",
    "    net = # ToDo, define second Dense-Layer\n",
    "    net = # ToDo, define second Dropout-Layer\n",
    "    net = # ToDo, define third Dense-Layer\n",
    "\n",
    "    model = Model(inputs, net, name='pointnet')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Transfer Learning of the PointNet\n",
    "\n",
    "Load the pretrained weigths for PointNet and change the output to four classes. Define all Hyperparameters and implement the train/test and your own learning rate scheduling algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = pointnet((None, 3), 6)\n",
    "pretrained_model.load_weights(\"pretrained-modelnet.h5\")\n",
    "\n",
    "# We need to replace the last layer, because it was trained to predict 6 classes instead of 4.\n",
    "embedding_layer = # ToDo find \"embedding layer\" (The layer before the old softmax layer)\n",
    "outputs = # ToDo Define new Output Layer\n",
    "new_model = Model(inputs=pretrained_model.inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define Hyperparameters\n",
    "SAMPLE_SIZE = \n",
    "EPOCHS = \n",
    "TRAIN_TEST_SPLIT = \n",
    "BATCH_SIZE ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split(samples, atFraction):\n",
    "    \"\"\"\n",
    "    Perform the train/test split.\n",
    "    \"\"\"\n",
    "    print(\"atFraction = \", atFraction)\n",
    "    \n",
    "    # ToDo perform the train/test split.\n",
    "    pass\n",
    "\n",
    "\n",
    "def reduce_learning_rate(epoch, currentLearningRate):\n",
    "    \"\"\"\n",
    "    Implements adaptive learning rate scheduling.\n",
    "    \"\"\"\n",
    "    # ToDo define your own learning rate schedule.\n",
    "    return currentLearningRate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, l_idx = initialize_dataset(\"ModelNet10/\")\n",
    "\n",
    "train_samples, validation_samples = random_split(df[df[\"is_train\"]], TRAIN_TEST_SPLIT)\n",
    "test_samples = df[~df[\"is_train\"]]\n",
    "\n",
    "print(\"Number of training samples: \", len(train_samples))\n",
    "print(\"Number of validation samples: \", len(validation_samples))\n",
    "print(\"Number of hold out test samples: \", len(test_samples))\n",
    "\n",
    "generator_training = ModelNetProvider(train_samples, batch_size=BATCH_SIZE, n_classes=4, sample_size=SAMPLE_SIZE)\n",
    "generator_validation = ModelNetProvider(validation_samples, batch_size=BATCH_SIZE, n_classes=4, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "\n",
    "new_model.compile(optimizer=\"adam\", loss='categorical_crossentropy',\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.LearningRateScheduler(reduce_learning_rate, verbose=1),\n",
    "    keras.callbacks.TensorBoard(\"logs/\", batch_size=BATCH_SIZE),\n",
    "    keras.callbacks.ModelCheckpoint(\"checkpoints/weights.{epoch:03d}-{val_loss:.2f}.h5\", save_weights_only=True, save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logir ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "new_model.fit_generator(generator=generator_training, epochs=EPOCHS, callbacks=callbacks, \n",
    "                    validation_data=generator_validation, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate model\n",
    "generator_test = ModelNetProvider(test_samples, batch_size=1, n_classes=4, sample_size=SAMPLE_SIZE)\n",
    "\n",
    "val_df = pd.DataFrame({\"prediction\": [], \"ground_truth\": []})\n",
    "for i in range(len(generator_test)):\n",
    "    X, y = generator_test[i]\n",
    "    prediction = new_model.predict(X)\n",
    "    val_df = val_df.append({\"prediction\": l_idx[np.argmax(prediction)],  \"ground_truth\": l_idx[np.argmax(y)]}, \n",
    "                  ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from ipywidgets import interact\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_context('talk')\n",
    "palette = sns.color_palette(n_colors=6)\n",
    "\n",
    "cm = metrics.confusion_matrix(val_df.ground_truth, val_df.prediction)\n",
    "confusionMatrix = pd.DataFrame(cm, index=l_idx, columns=l_idx)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "sns.heatmap(confusionMatrix, vmin=0, annot=True, fmt=\"d\", linewidth=.5, mask=(confusionMatrix==0), cmap=\"viridis\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Truth\")\n",
    "plt.ylim(4, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
