{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tin_rAmAzJvv"
      },
      "source": [
        "# Text Repräsentationen\n",
        "In diesem Teil des Labors wird es um eine Einfürung in Text Repräsentationen gehen. \n",
        "\n",
        "Wie Ihr vermutlich schon wisst, repräsentieren Computer Daten in Binärcode. Um Euch das Problem, das Computer mit Texten haben aufzuzeigen, hat euch der Computer eine Nachricht hinterlassen, die ihr Entschlüsseln müsst... "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Na68UqOJx8Jx"
      },
      "outputs": [],
      "source": [
        "binary_message = \"01001001 00100000 01110011 01110000 01100101 01100001 01101011 00100000 01100010 01101001 01101110 01100001 01110010 01111001 00100000 01101111 01101110 01101100 01111001 00100001 00001010 01001001 00100000 01101100 01101001 01101011 01100101 00100000 01101110 01110101 01101101 01100010 01100101 01110010 01110011 00101100 00100000 01101110 01101111 01110100 00100000 01110100 01100101 01111000 01110100 00101110\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "I_qao4JA1D0F"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I speak binary only!\n",
            "I like numbers, not text.\n"
          ]
        }
      ],
      "source": [
        "binary_chars = binary_message.split()\n",
        "message = \"\"\n",
        "for binary_char in binary_chars:\n",
        "    binary_char_integer = int(binary_char, 2)  # TODO\n",
        "    character = chr(binary_char_integer)\n",
        "    message += character\n",
        "print(message)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMwRdW3J1WQv"
      },
      "source": [
        "Um Maschinen Text verständlich zu machen, müssen wir die Texte in eine andere Repräsentation bringen. Eine Kurze Einführung bieten die beiden folgenden Warm Up Aufgaben."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioeYUuprcJOs"
      },
      "source": [
        "## Tokenization\n",
        "Eine der ersten elementaren Schritte, um eine natürliche Sprache mit einem Computer zu verarbeiten ist das Aufteilen eines Fließtextes in kleinere Teile. Meist werden Texte direkt in sogenannte Tokens geteilt (Tokenization). Ein Token ist oftmals ein einzelnes Wort, es kann aber auch aus mehreren Wörtern bestehen, wenn diese zusammen für eine Entität stehen, z.B. die Stadt „New York“. Neben der Aufteilung in Tokens ist es auch möglich, einen Text zuerst in Sätze zu splitten (Sentence Tokenization). Welche Kombination der Methoden angewendet wird, hängt von der Aufgabe ab. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HFy1A2zjkqL"
      },
      "source": [
        "### Warm Up 1: Tokenizer mit Regulären Ausdrücken\n",
        "\n",
        "Die erste kleine Aufgabe besteht darin mit einem regulären Ausdruck ein Wort-Tokenizer zu bauen. (Diese Aufgabe sollte nicht mehr als 5 min. dauern) Der Tokenizer muss auch nicht perfekt sein. \n",
        "\n",
        "> Eine kurze Auffrischung über Reguläre Ausdrücke findet sich in https://regexr.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vsACjYAEcHJc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Die', 'Ente', 'lacht', 'und', 'quakt.']"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "text = \"Die Ente lacht und quakt.\"\n",
        "\n",
        "tokenizer_regex = re.compile(r\"\\s+\")  # TODO\n",
        "tokenizer_regex.split(text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzaasIkDmK_r"
      },
      "source": [
        "Das Grundprinzip eines Tokenizers sollte euch nun klar sein. In der Praxis empfiehlt es sich allerdings, auf bestehende Tokenizer-Implementierungen zurückzugreifen.\n",
        "Im Python-Umfeld ist https://spacy.io/api/tokenizer oder https://www.nltk.org/api/nltk.tokenize.html sehr empfehlenswert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcOzQFueH9mC"
      },
      "source": [
        "# Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqqIUO30B46n"
      },
      "source": [
        "Der nächste Schritt, um mit Texten Maschinelles Lernen zu betreiben, ist das Umwandeln der Wörter in eine numerissche Repräsentation. In der nächsten Warm-Up Aufgabe wird es darum um Bag of Words gehen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-4dQzbG921p"
      },
      "source": [
        "## Warm Up 2: Bag of Words mit scikit-learn\n",
        "Ein Ansatz um Texte bzw. Tokens als Zahlen zu represäntieren ist ein Bag of Words. Hier wird jeder Text als Vektor dargestellt (Länge: Länge des Vokabulars). Jeder Eintrag im Vektor steht für ein Wort im Vokabular.\n",
        "\n",
        "In der nächsten Aufgabe wollen wir mit dem vorgegebenen Textkorpus, den Satz **\"Die Ente singt und quakt.\"** in einen bag-of-words-encoded Vector umwandeln.\n",
        "Dazu bietet sich der `CountVectorizer` an (siehe [Dokumentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html)).\n",
        "\n",
        "> Kleiner Tipp: Der `CountVectorizer` übernimmt diesmal die Tokenisierung."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ptA3RLihCfTj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['die' 'ente' 'lacht' 'quakt' 'singt' 'tanzt' 'und']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"Die Ente lacht und quakt.\", \"Die Ente singt und tanzt.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "vectorizer.fit_transform(corpus)  # TODO\n",
        "bag_of_words_encoded_sentence = vectorizer.get_feature_names_out()  # TODO\n",
        "print(bag_of_words_encoded_sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_WWHn0N_4CT"
      },
      "source": [
        "Im  nächsten Schritt gebt die Länge der Satzrepräsentation aus. \n",
        "\n",
        "Wie verändert sich die Länge, wenn sich der Textkorpus vergrößert. Was sind die Vorteile und Nachteile von Bag of Words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "44zUnF0P_zDZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "7\n"
          ]
        }
      ],
      "source": [
        "size_of_sentence_encoded=len(bag_of_words_encoded_sentence) #TODO\n",
        "print(size_of_sentence_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **_Die Länge ändert sich nur wenn der Textkorpus bisher unbekannte Wörter enthält, für die es noch keine Token gibt._**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbCGNsjiYuAT"
      },
      "source": [
        "## Word Embeddings\n",
        "\n",
        "Natürliche Sprachen bieten uns mannigfaltige Möglichkeiten, dieselben Inhalte auf unterschiedliche Art und Weise auszudrücken. Wenn wir mit Texten arbeiten, reicht es daher in der Regel nicht aus, nur die Anzahl und das Vorkommen von bestimmten Wörtern in Texten zu betrachten, weil neben des bloßen Vorkommen eines Wortes auch dessen Anordnung und dessen Bedeutung eine Rolle spielt. Als zusätzliche und wichtige Ebene kommt hier also die Semantik ins Spiel.\n",
        "\n",
        "Die Abbildung von Wörtern auf Vektoren erlaubt es uns, mit diesen zu rechnen und zum Beispiel Distanzen oder Ähnlichkeiten zu bestimmen. Embeddings haben den Vorteil, dass sie eine Dimensionsreduktion mit sich bringen und semantische Embeddings sorgen darüber hinaus dafür, dass \"verwandte\" Wörter einen geringen Abstand voneinander haben.\n",
        "\n",
        "Wir wollen uns im Folgenden zunächst mit Word2Vec beschäftigen und eine einfache Version des CBOW-Ansatzes selbst implementieren.\n",
        "\n",
        "Danach schauen wir uns Gensim als Bibliothek für Word-Embedding-Modelle an und werfen einen Blick auf Evaluationsmethoden für Embeddings sowie die ihnen inhärenten Biase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMup_-zoYuAY"
      },
      "source": [
        "## Aufgabe 1: Word2Vec CBOW\n",
        "> You shall know a word by the company it keeps.\n",
        ">\n",
        "> -- <cite>J. R. Firth</cite>\n",
        "\n",
        "Auf dem oben zitierten Prinzip beruhen die beiden als Word2Vec bekannt gewordenen Modelle CBOW und Skip Gram, die 2013 von [Tomas Mikolov et al.](https://arxiv.org/abs/1301.3781) bei Google entwickelt wurden.\n",
        "Erstgenanntes Modell werden wir im Folgenden in einer einfachen Form selbst implementieren."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xVNJ7AqYuAb"
      },
      "source": [
        "#### 1.1 Trainingsdaten\n",
        "Die Beschaffung und Aufbereitung von Trainingsdaten ist ein wichtiger Schritt in jeder NLP-Pipeline. Jetzt drücken wir uns mal davor und greifen auf einen Datensatz zu, den wir schonmal vorarb für Euch vorbereitet haben. Wir haben uns entschieden Alice im Wunderland [Gutenberg Project Alice im Wunderland](https://www.gutenberg.org/cache/epub/19778/pg19778.txt) zu nutzen um Word Embeddings zu trainieren. Der Datensatz beinhaltet die Sätze aus aus der Geschichte. \n",
        "\n",
        "Der vorverarbeitete Datensatz ist als pickle abgespeichert und findet sich in [hier](https://drive.google.com/file/d/1RfCivF-wHf33S7TMxjpT78923ADXreMi/view). Wir werden den googledrivedownloader nutzen, um die Datei zu laden. Ladet den Datensatz mit Pickle als Testdatensatz aus."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_fgETXxtJ-p8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RfCivF-wHf33S7TMxjpT78923ADXreMi\n",
            "To: /Users/sschwarzer/Desktop/repositories/ai-lab/nlp/download/alice_sentences.pkl\n",
            "100%|██████████| 219k/219k [00:00<00:00, 7.85MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'./download/alice_sentences.pkl'"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "\n",
        "gdown.download(\n",
        "    id=\"1RfCivF-wHf33S7TMxjpT78923ADXreMi\", output=\"./download/alice_sentences.pkl\", \n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Bmu1bOjcYuAf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "pkl = open(\"./download/alice_sentences.pkl\", \"rb\")\n",
        "data_sample = pickle.load(pkl) # pickle riiiick\n",
        "pkl.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAjugIzMYuAq"
      },
      "source": [
        "### 1.2 Datenvorbereitung\n",
        "Wir haben einen tokenisierten Datensatz, der aus Listen von Wörtern besteht. Jede Liste repräsentiert einen Satz. Unser Modell soll aber hinterher mit Zahlen hantieren und zwar entweder mit Wortindizes, die jedes Wort im Vokabular über einen eindeutige Nummer referenzierbar machen, oder mit One-hot-Vektoren, die als Labels dienen, mit denen der tatsächliche Output des Modells verglichen werden kann."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C3AIS0QfYuAt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word to id sample: [('wisperte', 0), ('hübscher', 1), ('trinkt', 2), ('sauberes', 3), ('knurren', 4), ('Geschrei', 5), ('höre', 6), ('aufgraben', 7), ('herum.«', 8), ('zusammengezogen', 9)] \n",
            "\n",
            "Id to word sample: [(0, 'wisperte'), (1, 'hübscher'), (2, 'trinkt'), (3, 'sauberes'), (4, 'knurren'), (5, 'Geschrei'), (6, 'höre'), (7, 'aufgraben'), (8, 'herum.«'), (9, 'zusammengezogen')] \n",
            "\n",
            "Documents as lists of integers: [1922, 433, 620, 2702, 1597, 2620, 4149]\n"
          ]
        }
      ],
      "source": [
        "# from collections import OrderedDict # FIX: Not required after Python 3.7\n",
        "\n",
        "# Beim Mapping von Wörtern zu IDs und umgekehrt sollte eine reproduzierbare Reihenfolge sichergestellt werden,\n",
        "# um das Modell später weitertrainieren und die Embedding-Matrix interpretieren zu können.\n",
        "# Diese Datenstruktur kann, aber muss nicht, als Basis dienen.\n",
        "vocabulary = set([word for sentence in data_sample for word in sentence]) # ! This is added\n",
        "unique_words = dict.fromkeys(vocabulary) # Liste alle Wörter, die in unserem data_sample vorkommen \n",
        "\n",
        "# Mapping von Wort zu ID\n",
        "word2id = {word : index for index, word in enumerate(unique_words.keys())} # TODO\n",
        "\n",
        "# Mapping von ID zu Wort\n",
        "id2word = {index : word for index, word in enumerate(unique_words.keys())} # TODO\n",
        "\n",
        "# Unser Data Sample, aber mit IDs statt Wörtern \n",
        "# [['der', 'hund', 'der', 'bellt'], ['die', 'katz', 'miaut']] => [[0, 1, 0, 2], [3, 4, 5]]\n",
        "numeric_docs = [[word2id[w] for w in doc] for doc in data_sample]\n",
        "\n",
        "print('Word to id sample:', list(word2id.items())[:10], '\\n')\n",
        "print('Id to word sample:', list(id2word.items())[:10], '\\n')\n",
        "print('Documents as lists of integers:', numeric_docs[0][:10])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKXEBPLqYuA1"
      },
      "source": [
        "Wir halten einige wichtige Parameter für unser Modell fest. Die Größe des Kontextfensters sowie die Länge der Embeddingvektoren können nach Bedarf angepasst werden. Für unsere Demo wählen wir kleine Werte."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ga_N0q-zYuA3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary Size: 4399\n"
          ]
        }
      ],
      "source": [
        "vocabulary_size = len(vocabulary)  # TODO\n",
        "embedding_size = 50  # Länge der Embeddingvektoren\n",
        "window_size = 2  # Größe des Kontextfensters. Wird nach rechts und links angewandt. Gesamter Kontext hier also 4 Wörter.\n",
        "\n",
        "print(\"Vocabulary Size:\", vocabulary_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uunq8JvnYuA-"
      },
      "source": [
        "### 1.3 Generator\n",
        "Um nicht alle Trainingsdaten auf einmal im Speicher halten zu müssen, schreiben wir uns eine Generatorfunktion, die Batches einer frei wählbaren Größe zurückgibt. Unser Ansatz ist dennoch nicht völlig speicherschonend, weil wir uns die Datengrundlage für die Generierung dieser Batches, nämlich die Integerlisten in `numeric_docs` sehr wohl im Speicher vorhalten. Darüber sehen wir aber großzügig hinweg.\n",
        "\n",
        "Die Generatorfunktion erzeugt zwei numpy-Arrays der Länge `batch_size`, von denen das eine Array eine Liste mit Indizes der Kontexwörter enthält, die der Embedding-Layer als Eingabe erwartet, und das andere die zugehörigen One-Hot-Encodings der Mittelwörter.\n",
        "\n",
        "Fiktives und vereinfachtes Beispiel:\n",
        "<pre><code>* Fenstergröße: 1\n",
        "* Batch-Size: 2\n",
        "* Wortindizes: 'die': 0, 'ente': 1, 'lacht': 2, 'und': '3, 'quakt': 4 (und damit Vokabulargröße 5)\n",
        "* Korpus (Auszug): [['die', 'ente', 'lacht', 'und', 'quakt'], ...]\n",
        "\n",
        "\n",
        "=> Rückgabe:  \n",
        "    contexts = [[0, 2], [1, 3]], \n",
        "    label_words = [[0, 1, 0, 0, 0], [0, 0, 1, 0, 0]]\n",
        "\n",
        "=> Alternative Rückgabe:\n",
        "    contexts = [['die', 'lacht'], ['ente', 'und']]\n",
        "    label_words = [['ente'], ['lacht']]\n",
        "</code></pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rItJIFM8YuBB"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-08 12:48:09.279696: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "from typing import Union, List, Iterator, Tuple\n",
        "\n",
        "# contexts = 2 * window_size * batch_size\n",
        "# label_words = batch_size * vocabulary_size\n",
        "\n",
        "def generate_context_word_batches(\n",
        "    corpus: Union[List[List[str]], List[List[int]]], # numeric_docs\n",
        "    window_size: int,\n",
        "    vocab_size: int, \n",
        "    batch_size: int, # definiert länge von contexts und label_words\n",
        ") -> Iterator[Tuple[np.ndarray, np.ndarray]]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        corpus: Tokenized text, split into list of strings for each sentence.\n",
        "        window_size (int): Number of words watched to the left/right of each word (aka. context).\n",
        "        vocab_size (int): Number of words in the vocabulary\n",
        "        batch_size (int): Length of the two returned arrays\n",
        "\n",
        "    Yields:\n",
        "        List[List[int]]: Two arrays of size batch_size with the first \n",
        "    \"\"\"\n",
        "    word2id = {word: index for index, word in enumerate(set(word for sentence in corpus for word in sentence))} # word2id \n",
        "    current_size = 0 # batch counter\n",
        "    while True:\n",
        "        # TODO: Here be dragons\n",
        "        ctx = []\n",
        "        labels = []\n",
        "        for sentence in corpus:\n",
        "            if (len(sentence) < window_size * 2 + 1):\n",
        "                continue\n",
        "            for label_index in range(window_size, len(sentence) - window_size):\n",
        "                ctx.append([word2id[sentence[label_index - i]] for i in range(-window_size, window_size + 1) if i != 0][::-1])\n",
        "                labels.append(np_utils.to_categorical(word2id[sentence[label_index]], len(word2id)))\n",
        "                current_size += 1\n",
        "                if (current_size == batch_size):\n",
        "                    current_size = 0\n",
        "                    contexts, label_words = np.array(ctx), np.array(labels) # needs to be a numpy array to use the model.fit() function\n",
        "                    ctx, labels = [], [] # reset memory\n",
        "                    yield np.array(contexts), np.array(label_words)  # zwei numpy arrays\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pseudocode\n",
        "\n",
        "```python\n",
        "\n",
        "for sentence in data_sample:\n",
        "    for (i = window_size; i < len(sentence) - window_size; i++) # loop over label_words in sentence\n",
        "        for word_index in range(-window_size, window_size) # loop over (left and right) contexts\n",
        "            current_word = sentence[word_index]    \n",
        "            if (word_index != 0): # if word is not label_word\n",
        "                X.add(current_word)\n",
        "            else:\n",
        "                Y.add(current_word)\n",
        "        current_size += 1\n",
        "        if (current_size == batch_size):\n",
        "            yield X, Y\n",
        "\n",
        "    sentence_counter += 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zQU_48R7YuBG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context (X): ['Erstes', 'Kapitel', 'in', 'den'] -> Target (Y): Hinunter\n",
            "Context (X): ['Kapitel', 'Hinunter', 'den', 'Kaninchenbau'] -> Target (Y): in\n",
            "Context (X): ['Hinunter', 'in', 'Kaninchenbau', '.'] -> Target (Y): den\n",
            "Context (X): ['Alice', 'fing', 'sich', 'zu'] -> Target (Y): an\n",
            "Context (X): ['fing', 'an', 'zu', 'langweilen'] -> Target (Y): sich\n",
            "Context (X): ['an', 'sich', 'langweilen', ';'] -> Target (Y): zu\n",
            "Context (X): ['sich', 'zu', ';', 'sie'] -> Target (Y): langweilen\n",
            "Context (X): ['zu', 'langweilen', 'sie', 'saß'] -> Target (Y): ;\n",
            "Context (X): ['langweilen', ';', 'saß', 'schon'] -> Target (Y): sie\n"
          ]
        }
      ],
      "source": [
        "# Schneller Test\n",
        "test_batch_size = 3\n",
        "test_window_size = 2\n",
        "\n",
        "batch_gen = generate_context_word_batches(corpus=numeric_docs, window_size=test_window_size, vocab_size=vocabulary_size, batch_size=test_batch_size)\n",
        "for i in range(0, 3): \n",
        "    x, y = next(batch_gen)\n",
        "    for j in range(0, test_batch_size):\n",
        "        print('Context (X):', [id2word[w] for w in x[j]], '-> Target (Y):', id2word[np.argwhere(y[j])[0][0]])  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhy4rehNYuBM"
      },
      "source": [
        "### 1.4 Definition des Models\n",
        "Als nächstes definieren wir unser Model. Dazu verwenden wir die Sequential API von Keras. Dieser [Blogpost](https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html) bietet nochmal eine anschauliche Erklärung, wie word2vec CBOW funktioniert.\n",
        "Das folgende Bild ist daraus und stellt die Architektur des Neuronalen Netzes dar:  \n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=500 />\n",
        "\n",
        "Weitere Informationen über den Embedding Layer finden sich [hier](https://keras.io/layers/embeddings/).\n",
        "\n",
        "> **Tipp:** Nehmt euch wirklich Zeit Embeddings zu verstehen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wdsQV9RDYuBO"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 4, 100)            439900    \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4399)              444299    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 884,199\n",
            "Trainable params: 884,199\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-11-08 12:48:16.213793: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import keras.backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, Lambda\n",
        "\n",
        "# ! https://github.com/nzw0301/keras-examples/blob/master/CBoW.ipynb\n",
        "\n",
        "#Modelldefinition\n",
        "cbow = Sequential()\n",
        "# input_length resolves around the context length\n",
        "cbow.add(Embedding(input_dim=vocabulary_size, output_dim=100, input_length=window_size*2)) #TODO\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embedding_size,)))\n",
        "# number of units equals every word out of the vocab, softmax to get a prob distribution\n",
        "cbow.add(Dense(vocabulary_size, activation=\"softmax\")) #TODO #Units #Activation\n",
        "cbow.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop')\n",
        "\n",
        "# Zusammenfassung\n",
        "print(cbow.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBoEx-w2YuBU"
      },
      "source": [
        "### 1.5 Training\n",
        "Jetzt wird es ernst: Wir trainieren unser Modell.\n",
        "Da wir eine eigene Generatorfunktion verwenden, müssen wir `steps_per_epoch` angeben. Überlegt euch was damit genau gemeint ist, wofür uns das nützt und was wir bei der Berechnung beachten müssen. Tipp: das Modell sollte pro Epoche alle Trainingsdaten sehen. Überlegt auch ob die Cbow-Fenstergröße einen einfluss auf diese Anzahl hat.\n",
        "\n",
        "Weil wir unser Modell gerne abspeichern möchten, zum Beispiel, um es später weiter zu trainieren, definieren wir eine Callback-Funktion, die das für uns übernimmt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-Ui-skXyYuBV"
      },
      "outputs": [],
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "model_checkpoint = ModelCheckpoint('embeddings.hd5', monitor='loss', verbose=1, save_best_only=True, save_weights_only=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lRuQzCB2YuBa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "88/89 [============================>.] - ETA: 0s - loss: 8.1600\n",
            "Epoch 1: loss improved from inf to 8.15636, saving model to embeddings.hd5\n",
            "INFO:tensorflow:Assets written to: embeddings.hd5/assets\n",
            "89/89 [==============================] - 3s 28ms/step - loss: 8.1564\n",
            "Epoch 2/5\n",
            "88/89 [============================>.] - ETA: 0s - loss: 7.2555\n",
            "Epoch 2: loss improved from 8.15636 to 7.25068, saving model to embeddings.hd5\n",
            "INFO:tensorflow:Assets written to: embeddings.hd5/assets\n",
            "89/89 [==============================] - 3s 35ms/step - loss: 7.2507\n",
            "Epoch 3/5\n",
            "89/89 [==============================] - ETA: 0s - loss: 6.5115\n",
            "Epoch 3: loss improved from 7.25068 to 6.51146, saving model to embeddings.hd5\n",
            "INFO:tensorflow:Assets written to: embeddings.hd5/assets\n",
            "89/89 [==============================] - 4s 44ms/step - loss: 6.5115\n",
            "Epoch 4/5\n",
            "89/89 [==============================] - ETA: 0s - loss: 6.3217\n",
            "Epoch 4: loss improved from 6.51146 to 6.32169, saving model to embeddings.hd5\n",
            "INFO:tensorflow:Assets written to: embeddings.hd5/assets\n",
            "89/89 [==============================] - 3s 33ms/step - loss: 6.3217\n",
            "Epoch 5/5\n",
            "89/89 [==============================] - ETA: 0s - loss: 6.2601\n",
            "Epoch 5: loss improved from 6.32169 to 6.26007, saving model to embeddings.hd5\n",
            "INFO:tensorflow:Assets written to: embeddings.hd5/assets\n",
            "89/89 [==============================] - 3s 34ms/step - loss: 6.2601\n"
          ]
        }
      ],
      "source": [
        "epochs = 5  # (kann gerne erhöht werden)\n",
        "batch_size = 300\n",
        "# das Modell sollte pro Epoche alle Trainingsdaten sehen. Überlegt auch ob die Cbow-Fenstergröße einen einfluss auf diese Anzahl hat.\n",
        "steps_per_epoch = sum(\n",
        "    [max(len(sentence) - (window_size * 2 + 1), 0) for sentence in data_sample]\n",
        ") // batch_size # TODO Dies ist die Anzahl der Generatoraufrufe pro Epoche. An sich ist es sinnvoll, diesen Wert auf ´#samples//batch_size`` zu setzen, damit das Modell pro Epoche alle Trainingsdaten sieht\n",
        "\n",
        "# deprecated, .fit supports generators\n",
        "\n",
        "# cbow.fit_generator(\n",
        "#     generator=generate_context_word_batches(\n",
        "#         corpus=data_sample,\n",
        "#         window_size=window_size,\n",
        "#         vocab_size=vocabulary_size,\n",
        "#         batch_size=batch_size,\n",
        "#     ),  # TODO\n",
        "#     epochs=epochs,  # TODO\n",
        "#     steps_per_epoch=steps_per_epoch,  # TODO\n",
        "#     callbacks=[model_checkpoint],  # TODO\n",
        "# )\n",
        "\n",
        "# y and batch_size should not be specified when using a generator\n",
        "history = cbow.fit(\n",
        "    x=generate_context_word_batches(\n",
        "        corpus=data_sample,\n",
        "        window_size=window_size,\n",
        "        vocab_size=vocabulary_size,\n",
        "        batch_size=batch_size,\n",
        "    ),\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    callbacks=[model_checkpoint],\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMIqy_-fYuBf"
      },
      "source": [
        "### 1.6 Test\n",
        "Nachdem wir unser Modell nur sehr kurz und nur auf wenigen Daten trainiert haben, ist davon auszugehen, dass die Ergebnisse nicht optimal sind. Einen kurzen Blick wollen wir dennoch riskieren.\n",
        "\n",
        "Dazu extrahieren wir zunächst die Gewichte aus dem Embedding-Layer und schauen sie uns auszugsweise an."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "m7x0IPFqYuBh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>wisperte</th>\n",
              "      <td>0.019688</td>\n",
              "      <td>-0.020906</td>\n",
              "      <td>-0.010620</td>\n",
              "      <td>-0.015431</td>\n",
              "      <td>-0.033278</td>\n",
              "      <td>0.019629</td>\n",
              "      <td>-0.028774</td>\n",
              "      <td>-0.002464</td>\n",
              "      <td>-0.049655</td>\n",
              "      <td>0.055011</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054802</td>\n",
              "      <td>0.048052</td>\n",
              "      <td>-0.002675</td>\n",
              "      <td>0.004507</td>\n",
              "      <td>-0.064263</td>\n",
              "      <td>-0.001790</td>\n",
              "      <td>0.048621</td>\n",
              "      <td>-0.060419</td>\n",
              "      <td>0.002684</td>\n",
              "      <td>-0.024934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hübscher</th>\n",
              "      <td>-0.031907</td>\n",
              "      <td>0.003123</td>\n",
              "      <td>0.045882</td>\n",
              "      <td>0.011462</td>\n",
              "      <td>-0.018991</td>\n",
              "      <td>0.062678</td>\n",
              "      <td>0.014064</td>\n",
              "      <td>-0.045243</td>\n",
              "      <td>-0.050458</td>\n",
              "      <td>-0.013482</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.007344</td>\n",
              "      <td>-0.028524</td>\n",
              "      <td>-0.031760</td>\n",
              "      <td>-0.049186</td>\n",
              "      <td>0.010973</td>\n",
              "      <td>-0.002943</td>\n",
              "      <td>0.051939</td>\n",
              "      <td>-0.011931</td>\n",
              "      <td>-0.009198</td>\n",
              "      <td>0.043524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>trinkt</th>\n",
              "      <td>-0.009499</td>\n",
              "      <td>0.014846</td>\n",
              "      <td>-0.006785</td>\n",
              "      <td>-0.020151</td>\n",
              "      <td>0.053994</td>\n",
              "      <td>0.013552</td>\n",
              "      <td>0.004127</td>\n",
              "      <td>-0.039954</td>\n",
              "      <td>-0.039460</td>\n",
              "      <td>-0.018305</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031358</td>\n",
              "      <td>0.017762</td>\n",
              "      <td>0.047564</td>\n",
              "      <td>0.019043</td>\n",
              "      <td>0.014842</td>\n",
              "      <td>0.014977</td>\n",
              "      <td>0.063935</td>\n",
              "      <td>-0.040929</td>\n",
              "      <td>0.030067</td>\n",
              "      <td>0.046317</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sauberes</th>\n",
              "      <td>0.014415</td>\n",
              "      <td>-0.033019</td>\n",
              "      <td>-0.016654</td>\n",
              "      <td>0.005282</td>\n",
              "      <td>-0.021218</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>-0.019148</td>\n",
              "      <td>-0.019716</td>\n",
              "      <td>-0.035020</td>\n",
              "      <td>0.060449</td>\n",
              "      <td>...</td>\n",
              "      <td>0.059903</td>\n",
              "      <td>0.065154</td>\n",
              "      <td>-0.033325</td>\n",
              "      <td>-0.001673</td>\n",
              "      <td>0.039711</td>\n",
              "      <td>-0.013402</td>\n",
              "      <td>0.038106</td>\n",
              "      <td>-0.062765</td>\n",
              "      <td>0.040553</td>\n",
              "      <td>-0.024895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>knurren</th>\n",
              "      <td>0.005091</td>\n",
              "      <td>0.007309</td>\n",
              "      <td>-0.033350</td>\n",
              "      <td>-0.003477</td>\n",
              "      <td>0.059276</td>\n",
              "      <td>0.056922</td>\n",
              "      <td>0.013418</td>\n",
              "      <td>-0.023101</td>\n",
              "      <td>-0.001393</td>\n",
              "      <td>-0.013006</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020489</td>\n",
              "      <td>0.024506</td>\n",
              "      <td>-0.024635</td>\n",
              "      <td>-0.064822</td>\n",
              "      <td>0.028294</td>\n",
              "      <td>0.005668</td>\n",
              "      <td>0.007854</td>\n",
              "      <td>-0.027124</td>\n",
              "      <td>0.060357</td>\n",
              "      <td>0.022493</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                0         1         2         3         4         5   \\\n",
              "wisperte  0.019688 -0.020906 -0.010620 -0.015431 -0.033278  0.019629   \n",
              "hübscher -0.031907  0.003123  0.045882  0.011462 -0.018991  0.062678   \n",
              "trinkt   -0.009499  0.014846 -0.006785 -0.020151  0.053994  0.013552   \n",
              "sauberes  0.014415 -0.033019 -0.016654  0.005282 -0.021218  0.004291   \n",
              "knurren   0.005091  0.007309 -0.033350 -0.003477  0.059276  0.056922   \n",
              "\n",
              "                6         7         8         9   ...        90        91  \\\n",
              "wisperte -0.028774 -0.002464 -0.049655  0.055011  ...  0.054802  0.048052   \n",
              "hübscher  0.014064 -0.045243 -0.050458 -0.013482  ... -0.007344 -0.028524   \n",
              "trinkt    0.004127 -0.039954 -0.039460 -0.018305  ...  0.031358  0.017762   \n",
              "sauberes -0.019148 -0.019716 -0.035020  0.060449  ...  0.059903  0.065154   \n",
              "knurren   0.013418 -0.023101 -0.001393 -0.013006  ... -0.020489  0.024506   \n",
              "\n",
              "                92        93        94        95        96        97  \\\n",
              "wisperte -0.002675  0.004507 -0.064263 -0.001790  0.048621 -0.060419   \n",
              "hübscher -0.031760 -0.049186  0.010973 -0.002943  0.051939 -0.011931   \n",
              "trinkt    0.047564  0.019043  0.014842  0.014977  0.063935 -0.040929   \n",
              "sauberes -0.033325 -0.001673  0.039711 -0.013402  0.038106 -0.062765   \n",
              "knurren  -0.024635 -0.064822  0.028294  0.005668  0.007854 -0.027124   \n",
              "\n",
              "                98        99  \n",
              "wisperte  0.002684 -0.024934  \n",
              "hübscher -0.009198  0.043524  \n",
              "trinkt    0.030067  0.046317  \n",
              "sauberes  0.040553 -0.024895  \n",
              "knurren   0.060357  0.022493  \n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from keras.models import load_model\n",
        "\n",
        "cbow = load_model(\"./embeddings.hd5\") # TODO: Model laden\n",
        "embedding_weights = cbow.get_layer(\"embedding\").get_weights()[0] # TODO: Auf Embedding Layer (1. Layer) des Modells zugreifen und dort die Gewichtsmatrix extrahieren\n",
        "\n",
        "pd.DataFrame(embedding_weights, index=list(id2word.values())).head() # TODO: Was wollen wir anschauen?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BnUH3l6YuBm"
      },
      "source": [
        "Da durch scharfes Hinsehen nicht unmittelbar zu erkennen ist, wie gut unsere Embeddings schon sind, machen wir stichprobenartige Tests. Dazu wählen wir einige Wörter und berechnen für deren Embeddings die Ähnlichkeit mit allen anderen Embedding-Vektoren in unserer Gewichtsmatrix. Anschließend lassen wir uns die fünf ähnlichsten Wörter ausgeben.\n",
        "Überlegt euch welches Distantzmaß für den Vergleich von Vektoren genutzt werden kann. Tipp: Die Cosinusähnlichkeit könnte damit was zu tun haben. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "lwHpAG9lYuBo"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity # Tipp: from sklearn.metrics.pairwise import ?\n",
        "\n",
        "sample_terms = ['Alice', 'Hut', 'Kaninchen','Kaninchenbau']\n",
        "sample_embeddings = [embedding_weights[word2id[term]] for term in sample_terms] # TODO\n",
        "\n",
        "# Berechne die paarweisen Distanzen zwischen Beispielwörtern und Gesamtvokabular\n",
        "distance_matrix = cosine_similarity(sample_embeddings, embedding_weights)# TODO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E1HEcW_bYuBt"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Alice': ['gegeben.', 'zurückgekommen', 'Worauf', 'Woran', 'Mensch'],\n",
              " 'Hut': ['Hierauf', 'Mensch', 'Gründe', 'zurückgekommen', 'verursachte'],\n",
              " 'Kaninchen': ['gegeben.', 'schenken', 'Mensch', 'Woran', 'zurückgekommen'],\n",
              " 'Kaninchenbau': ['gegeben.',\n",
              "  'Mensch',\n",
              "  'bekommt',\n",
              "  'eleganten',\n",
              "  'zurückgekommen']}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Zeige die top fünf ähnlichsten Wörter zu unseren Beispielwörtern \n",
        "similar_words = {sample_term: [id2word[idx] for idx in distance_matrix[index].argsort()[1:6]] \n",
        "                   for index, sample_term in enumerate(sample_terms)}\n",
        "\n",
        "similar_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LM3qNiNwHyFl"
      },
      "source": [
        "Seid ihr zufrieden mit den ähnlichen Wörtern? Woran kann es liegen, dass die Wörter nicht immer unbedingt Sinn ergeben?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt-nOPurYuBz"
      },
      "source": [
        "## Aufgabe 2: Gensim als Wrapper für Word2Vec-Modelle\n",
        "Embedding-Layer begegnen einem in der Praxis in der Tat häufig. In der Regel aber nicht als Bestandteile von reinen Word Embedding-Trainingsmodellen, sondern als erster Layer für Modelle mit anderen Aufgaben. Die Embeddings werden dann entweder mit vorberechneten Werten initalisiert und/oder werden im Training des Modells für die Downstream-Aufgabe (Textklassifikation, Übersetzung, ...) mittrainiert.\n",
        "\n",
        "Eine komfortable Möglichkeit, eigene Word2Vec-Modelle zu trainieren, bietet [Gensim](https://radimrehurek.com/gensim/models/word2vec.html), eine Bibliothek, die für diese Modelle auch Wrapper bereitstellt, um komfortabler an die Embeddings zu kommen und mit diesen zu arbeiten.\n",
        "\n",
        "Wir wollen uns im Folgenden einen kleinen Ausschnitt der Möglichkeiten, die Gensim bietet, anschauen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q86D9vsXFN2e"
      },
      "source": [
        "### 2.0 Vorbereitung\n",
        "Wir werden mit vortrainierten Google-News-Embeddings arbeiten, die ihr mit dem Code in den nächsten beiden Zellen herunterladen könnt. Falls das nicht funktionieren sollte, findet ihr die vortrainierten Embeddings [hier](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WXAQA-n5FgsQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gdown in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (4.5.3)\n",
            "Requirement already satisfied: filelock in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from gdown) (3.6.0)\n",
            "Requirement already satisfied: tqdm in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from gdown) (4.11.1)\n",
            "Requirement already satisfied: requests[socks] in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from gdown) (2.27.1)\n",
            "Requirement already satisfied: six in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.2.post1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.12)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/sschwarzer/opt/anaconda3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hO0e4QiDEeUm"
      },
      "outputs": [],
      "source": [
        "from gdown import download\n",
        "import os.path\n",
        "\n",
        "if not os.path.isfile(\"./download/GoogleNews-vectors-negative300.bin\"): \n",
        "    download(\n",
        "        id=\"1Fl11N_cX1RfJmTHV1C-RGIsjWeIZjbTN\",\n",
        "        output=\"./download/GoogleNews-vectors-negative300.bin.gz\",\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpLcc8yYuB0"
      },
      "source": [
        "### 2.1 Word2Vec-Model laden\n",
        "Die Vektoren haben eine Länge von 300.\n",
        "Zieht euch die Embeddings über den oben angegebenen Link und verwendet gensim, um sie anschließend zu laden.\n",
        "\n",
        "**Hinweis**: Es kann einen Moment dauern, bis das Dictionary, das von Wort auf Embedding abbildet, erzeugt ist. Im Zweifel ein ```limit``` angeben und nur die ersten 1,5 Mio. Embeddings laden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "uiXE550K0CfC"
      },
      "outputs": [],
      "source": [
        "if not os.path.isfile(\"./download/GoogleNews-vectors-negative300.bin\"): \n",
        "    !gunzip ./download/GoogleNews-vectors-negative300.bin.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Em5AdDKlYuB2"
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1252212729.py, line 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/var/folders/3v/78hxlk7s49sfnp3vnprqd5p40000gn/T/ipykernel_24757/1252212729.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    embeddings =  # TODO\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "import gensim\n",
        "\n",
        "embeddings = gensim # TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wk7S_3n4YuB6"
      },
      "source": [
        "Aus Spaß an der Freude können wir nun schauen, wie gut unser trainiertes Modell ist bzw. ob es bestimmte von Menschen wahrgenommene Analogien bestätigt. \n",
        "\n",
        "Die Analogien sind [hier](https://github.com/nicholas-leonard/word2vec/blob/master/questions-words.txt) zu finden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r89GRNDrYuB6"
      },
      "outputs": [],
      "source": [
        "from gensim.test.utils import datapath\n",
        "\n",
        "embeddings.evaluate_word_analogies(datapath(\"questions-words.txt\"), restrict_vocab=30000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCI5ETPYYuB_"
      },
      "source": [
        "### 2.2 Spaß mit Semantik\n",
        "Im Folgenden wollen wir uns mit dem Mehrwert beschäftigen, den semantische Embeddings bieten. Für weitere Inspiration siehe zum Beispiel [hier](https://www.machinelearningplus.com/nlp/gensim-tutorial/) und die [Doku](https://radimrehurek.com/gensim/models/keyedvectors.html).\n",
        "\n",
        "Mit semantischen Vektoren lassen sich zum Beispiel folgende Fragen beantworten:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mgxhguPYuCA"
      },
      "outputs": [],
      "source": [
        "# Welche Stadt ist das New York Deutschlands? (Hinweis: 'New_York' ist als Token in den Embeddings enthalten)\n",
        "print('Das deutsche New York ist: {}\\n'.format(# TODO)))\n",
        "\n",
        "# Was ist Emacs besonders ähnlich?\n",
        "print('Ähnlichste Begriffe zu \"Emacs\": {}\\n'.format(# TODO))\n",
        "\n",
        "# Und wie sieht es mit Vim aus?\n",
        "print('Ähnlichste Begriffe zu \"Vim\": {}\\n'.format(# TODO))\n",
        "\n",
        "# Wer ist eigentlich der Mozart der Naturwissenschaft?\n",
        "print('Der Mozart der Naturwissenschaft ist: {}\\n'.format(# TODO))\n",
        "\n",
        "# Welches Wort verhält sich zu 'singing' wie 'burnt' zu 'burning'?\n",
        "print('burning:burnt wie singing:{}\\n'.format(# TODO))\n",
        "\n",
        "# Sind sich Deutschland und Frankreich ähnlicher oder Deutschland und Kanada?\n",
        "print('Ähnlichkeit DE, FR: {}'.format(# TODO))\n",
        "print('Ähnlichkeit DE, CAN: {}'.format(# TODO))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3g5ivIxYuCE"
      },
      "source": [
        "Bei der Interpretation der Ergebnisse ist jedoch Vorsicht geboten: Es werden zwar semantische Beziehungen abgebildet, aber die entsprechen möglicherweise nicht immer den Erwartungen.\n",
        "\n",
        "Wie ähnlich sind sich zum Beispiel \"Leben\" und \"Tod\", \"kalt\" und \"warm\", \"Norden\" und \"Süden\"?\n",
        "\n",
        "Sind die Ergebnisse wie erwartet? Warum (nicht)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXgw0wIMYuCF"
      },
      "outputs": [],
      "source": [
        "# TODO: Ähnlichkeiten berechnen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPjd7_zJYuCJ"
      },
      "source": [
        "Die Embeddings sind nicht neutral, sondern spiegeln die Beziehungen wieder, die sich in den Trainingsdaten finden lassen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LLI9StHLYuCK"
      },
      "outputs": [],
      "source": [
        "# Wird Wissenschaft von Frauen oder Männern gemacht?\n",
        "print('Wissenschaft wird gemacht von: {}'.format(# TODO))\n",
        "# Sind Mörder eher Schwarze, Weiße oder Asiaten?\n",
        "print('Mörder sind: {}'.format(# TODO))\n",
        "# Was bleibt vom Mann, wenn die Intelligenz abgezogen wird?\n",
        "print('Mann ohne Intelligenz: {}'.format(# TODO))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "05_1_text_representation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "119d84cb8c3b84b6d48a93e2933b395fcb363c6d1cda3c0de52c91f1ece8e0fb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
