{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "06_2_machine_trans.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCWM2yomH_Vi",
        "colab_type": "text"
      },
      "source": [
        "# Machine Translation mit Encoder-Decoder Modell\n",
        "\n",
        "In diesem Notebook m√∂chten wir uns mit der K√∂nigsdisziplin des Natural Language Processings besch√§ftigen, der maschinellen √úbersetzung. Vermutlich gibt es keine NLP-Anwendung, die einerseits vielen bekannt ist, anderseits durch Deep Learning und neuronalen Netzen einen solchen Aufschwung bekommen hat.\n",
        "\n",
        "Vor dem Siegeszug der neuronalen Netze, wurde maschinelle √úbersetzung deshalb als schwierig angesehen, weil diese Aufgabe alle Teilaspekte von Sprache beinhaltet. Neben grammatikalischer Korrektheit, sollen maschinell √ºbersetzte Texte den Sinn des Originaltextes wiedergeben und dar√ºberhinaus auch den Subtext, wie Ironie, Witz, erfassen. Ob letzteres einfach gelingt, sei hier mal dahingestellt.\n",
        "\n",
        "Deshalb m√∂chten wir in diesem Notebook ein ML Modell aufbauen, dass englische S√§tze auf Deutsch √ºbersetzt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOW8B0f4wlBG",
        "colab_type": "text"
      },
      "source": [
        "## 0. GPU-Nutzung\n",
        "Diesmal ist es empfehlenswert f√ºrs Training eine GPU zu nutzen. Bevor Ihr das Training startet, k√∂nnt ihr mit dem folgenden Code √ºberpr√ºfen ob in Colab eine GPU registriert ist. Bitte nutzt die GPU erst, wenn Ihr mit der Implementierung fertig seid, um Ressourcen zu sparen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knXblt-g61ZH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppL7DDGiH_Vk",
        "colab_type": "text"
      },
      "source": [
        "## 1. Daten\n",
        "Im Vergleich zu den bisherigen Notebooks sind die Daten f√ºr Machine Translation relativ unspannend. Es handelt sich um Englisch-Deutsche-Satzpaare. Daher m√∂chten wir uns hiermit nicht allzu lange aufhalten. Ladet euch wie gewohnt die Daten aus Google Drive. (Falls das nicht klappt: [hier](www.manythings.org/anki/deu-eng.zip) findet ihr die Daten).\n",
        "\n",
        "\n",
        "Danach wollen wir die Daten einlesen. Das Format ist einfach: pro Zeile steht ein Satzpaar getrennt von einem Tab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLdnhWpOx5va",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install googledrivedownloader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmiFL3nKxdih",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google_drive_downloader import GoogleDriveDownloader as gdd\n",
        "\n",
        "gdd.download_file_from_google_drive(file_id='1ECd2plUitjNkU-xdo9bWCNRcscTgOoVu',\n",
        "                                    dest_path='./download/deu-eng.zip',\n",
        "                                    unzip=True,\n",
        "                                    overwrite=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sjSp9hsH_Vm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('./download/deu.txt', 'r') as f:\n",
        "    lines = f.readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFiRW0mfH_Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs = [tuple(l.split(\"\\t\")) for l in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3RlV5QcH_V6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs_cleaned = [(e, g.strip()) for e, g, _ in pairs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2-L6iVTH_WA",
        "colab_type": "text"
      },
      "source": [
        "Nutzt den `TweetTokenizer` von NLTK, um die englischen und deutschen S√§tze in Tokens umzuwandeln.\n",
        "Damit unser Modell wei√ü wo Anfang und Ende der deutschen S√§tze sind, m√ºssen wir noch ein spezielles Start- (üè≥Ô∏è) und Endsymbol (üè¥) einf√ºgen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ykawOhDhM3x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfBwZliYH_WB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pairs_cleaned_with_marker = [... for e, g in pairs_cleaned] #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqLAIs2dH_WI",
        "colab_type": "text"
      },
      "source": [
        "Sammelt nun alle Tokens, die in den englischen bzw. den deutschen S√§tzen vorkommen, in jeweils einem Set ab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLphA0xiH_WJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_tokens = set()\n",
        "german_tokens = set()\n",
        "\n",
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Le5C1aRH_WP",
        "colab_type": "text"
      },
      "source": [
        "Um unser Modell sp√§ter mit passenden Eingabedaten f√ºttern zu k√∂nnen, m√ºssen wir nun noch die jeweiligen Vokabularindices f√ºr den Encoder und den Decoder bestimmen. Wie gewohnt nehmen wir daf√ºr Dictionaries, die jeweils die Tokens auf den richtigen Index abbilden. \n",
        "Damit wir sp√§ter unsere Batches padden k√∂nnen, f√ºgen wir noch ein `PADDING`-Token ein, das den Index `0` hat."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w71UaHcHH_WW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_token_index = #TODO\n",
        "german_token_index = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z066x6aGT-C9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "assert english_token_index[\"PADDING\"] == 0\n",
        "assert german_token_index[\"PADDING\"] == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF6m8A8u5NA4",
        "colab_type": "text"
      },
      "source": [
        "Speichert die L√§ngen des Vokabulars in den Variablen  `english_tokens_len` und `german_tokens_len` ab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FgZc5diH_WQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_tokens_len = #TODO\n",
        "german_tokens_len = #TODO\n",
        "print(f\"Englisch: {english_tokens_len}\")\n",
        "print(f\"Deutsch: {german_tokens_len}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av54thIxUK_Z",
        "colab_type": "text"
      },
      "source": [
        "Um die Verarbeitung der S√§tze zu vereinfachen, speichern wir nun keine W√∂rter mehr, sondern Wortindices. Im n√§chsten Schritt sollt ihr deshalb `pairs_cleaned_with_marker` in das Format z.B. `[([1,4,6,7,...],[7,1,2,8,...]),([1,5,9,7,...],[2,7,9,8,...]), ...]` umwandeln. (Tokens --> Zahlen)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfI-kysYa24y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sentences_idx=[... for e,g in pairs_cleaned_with_marker] #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZmLgG77H_Wa",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Data Generator\n",
        "\n",
        "Wir haben nun alle Dimensionen bestimmt, um die Trainingsdaten zu definieren. Daf√ºr bauen wir uns mal wieder einen Data Generator. Da wir relativ viele Daten haben, m√ºssen wir die Daten pro Batch generieren und in das Model laden. Da wir diesmal ein relativ kompliziertes Machine Learning Model bauen, haben wir f√ºr euch den Generator schon fast ganz vorimplementiert. \n",
        "Ihr m√ºsst nur noch die methode `__data_generation()` implementieren.\n",
        "\n",
        "Wir nutzen sp√§ter `model.fit_generator()`, in der die Funktion `__getitem()__` aufgerufen wird, um einen Batch in das Modell zu laden."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbEsH55aVs0A",
        "colab_type": "text"
      },
      "source": [
        "Wir erzeugen uns zuerst Daten, um unseren Encoder und Decoder zu trainieren. Sp√§ter werden wir die Architektur umbauen, um unsere eigentliche Predictions zu machen.\n",
        "Um unser Model zu trainieren bekommt es sowohl englische als auch deutsche Satzpaare als Input:\n",
        "\n",
        "```\n",
        "encoder_input=[[\"hi\", \",\", \"my\",\"name\",\"is\", \"tom\"]]\n",
        "deoder_input=[[\"üè≥Ô∏è\", \"hallo\", \",\", \"mein\",\"name\",\"ist\", \"tom\", \"üè¥\"]]\n",
        "```\n",
        "Die Ausgabe unseres Models soll wieder der Deutsche Satz sein, aber diesmal bauen wir ein offset von 1 ein, da wir immer gegeben den Kontext das n√§chste Wort vorhersagen m√∂chten:\n",
        "\n",
        "```\n",
        "decoder_outputs=[[\"hallo\", \",\", \"mein\",\"name\",\"ist\", \"tom\", \"üè¥\"]]\n",
        "```\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Da unser Model nat√ºrlich keine W√∂rter, sondern Zahlen nimmt soll die R√ºckgabe eines Batches in folgendem Format erfolgen:\n",
        "  \n",
        "  `([encoder_inputs, decoder_inputs], decoder_outputs)`\n",
        "\n",
        "* `encoder_inputs` ist ein Batch unserer *englischen* S√§tze im Wortindex-Format (englisches Vokabular),\n",
        "z.B. ```[[idx_1,idx_2][idx_2,idx_3],[...]]```\n",
        "\n",
        "* `decoder_inputs` ist ein Batch unserer *deutschen* S√§tze im Wortindex-Format (deutsches Vokabular),\n",
        "z.B. ```[[idx_1,idx_2][idx_2,idx_3],[...]]```\n",
        "\n",
        "Um eine Sequenz zu √ºbersetzen, wollen wir sp√§ter die  `decoder_outputs` vorhersagen. Hierbei handelt es sich um eine Klassifikation auf dem gesamten deutschen Vokabular. \n",
        "\n",
        "* Wir bauen hier noch ein offset von `1` ein und die `decoder_outputs` sehen dann f√ºr einen Batch bspw. folgenderma√üen aus:\n",
        "```[[[0,0,0],[1,0,0],[0,1,0]],[[0,0,0],[0,1,0],[0,0,1]],[...]]```\n",
        "\n",
        "**Padding**: Wir hatten zuvor das `PADDING`-Token angesprochen. Ein Problem, dass man h√§ufig bei Texten hat, ist, dass diese unterschiedlich lang sind. Da unser Netz nur mit Tensoren umgehen kann und es im Batch immer eine gleiche L√§nge erwartet, m√ºssen wir alle zu kurzen S√§tze  \"padden\". Wir f√ºllen deshalb alle S√§tze, die k√ºrzer sind als der l√§ngste Satz mit `0` (bzw. mit dem `PADDING`-Token) auf. \n",
        "\n",
        "> Wichtig: Die `encoder_inputs` werden links und die `decoder_inputs` rechts gepadded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-ARfvUUXrsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "class DataGenerator(keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, sentences_idx, german_tokens_len ,batch_size=64, shuffle=True):\n",
        "        'Initialization'\n",
        "        self.batch_size = batch_size\n",
        "        self.sentences_idx = sentences_idx\n",
        "        self.german_tokens_len=german_tokens_len\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return int(np.floor(len(self.sentences_idx) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Generate indexes of the batch\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "\n",
        "        # Find list of Indexes for batch\n",
        "        list_IDXs_batch = [self.sentences_idx[k] for k in indexes]\n",
        "\n",
        "        # Generate data\n",
        "        encoder_inputs, decoder_inputs, decoder_outputs=self.__data_generation(list_IDXs_batch)\n",
        "\n",
        "        return ([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        'Updates indexes after each epoch'\n",
        "        self.indexes = np.arange(len(self.sentences_idx))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "    def __data_generation(self, list_IDXs_batch):\n",
        "        '''\n",
        "        Generates data containing batch_size samples.\n",
        "        Padds the sentences in a batch to the longest sentence in a batch.\n",
        "        encoder_input are left padded and decoder_input are right padded.\n",
        "        The decoder output is a List of one-hot-encoded Vectors for each Sentence.\n",
        "        list_IDXs_batch --> [([english_idxs],[german_idxs]),([english_idxs],[german_idxs]),...]\n",
        "        '''\n",
        "\n",
        "        #TODO\n",
        "\n",
        "\n",
        "        return encoder_input, decoder_input, decoder_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkl1UmyKZmSU",
        "colab_type": "text"
      },
      "source": [
        "Nutzt nun den fertigen Generator und splittet `sentences_idx` in einen Train- und Validation-Datensatz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3FjZ3qs1_Ou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "# TODO\n",
        "\n",
        "train, val = # TODO\n",
        "train_samples_len= # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFhKjGNbH_Wr",
        "colab_type": "text"
      },
      "source": [
        "## 2. Sequence-to-sequence Modelle\n",
        "\n",
        " \n",
        "F√ºr unser Machine Translation Model bauen wir in diesem Teil ein Seq2Seq-Modell auf, das aus einem Encoder und einem Decoder besteht. Diese basieren jeweils wieder auf `LSTM`-Zellen, um Abh√§ngigkeiten in den Sequenzen zu lernen. \n",
        "\n",
        "Hierf√ºr nutzen wir die Functional API von Keras. Im Gegensatz zu der Sequence-API, definiert man hier ein Modell, sodass eine Layer mit der jeweiligen Vorg√§ngerlayer aufgerufen wird und ein Tensor zur√ºckgegeben wird, z.B:\n",
        "\n",
        "```\n",
        "a = Input(...)\n",
        "b = Dense(...)\n",
        "t = b(a)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gXjODo4H_Ws",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Nutzt f√ºr das LSTM und das sp√§tere Training folgende Parameter:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpywKeozuJ_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_len = 50      # Length of the vector that we will be returned from the embedding layer\n",
        "latent_dim    = 256     # Hidden layers dimension \n",
        "batch_size    = 64      # Batch size\n",
        "epochs        = 50      # Number of epochs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQBHt3Ekycws",
        "colab_type": "text"
      },
      "source": [
        "Bevor wir mit dem Encoder starten, erstellt Euch mit dem zuvor gebauten DataGenerator und den gesplitteten Daten je einen Generator f√ºrs Trainieren und einen f√ºrs Validieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZ5uqsKLAH2E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_train = DataGenerator(#TODO)\n",
        "generator_val = DataGenerator(#TODO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jygRjRqwyaG6",
        "colab_type": "text"
      },
      "source": [
        "### 2.1 Encoder\n",
        "\n",
        "Auch das komplexeste Modell f√§ngt mit einer Eingabe ein. \n",
        "\n",
        "Definiert einen Input-Layer, der als Eingabe englische S√§tze beliebiger L√§nge als Vektoren akzeptiert. Eine Komponente im Vektor repr√§sentiert je ein Wort. \n",
        "\n",
        "* Da wir sp√§ter auch kleinere Batchgr√∂√üen (z.B. nur einen Satz) vorhersagen m√∂chten, setzt die Batch-Size des Input-Layers auf `None`.\n",
        "* Da wir jeweils nur in einem Batch padden, um effizienter zu trainieren, kommt es vor, dass die S√§tze in unterschiedlichen Batches auf unterschiedliche L√§ngen gepadded werden. Damit ist es nicht m√∂glich eine eindeutige Satzl√§nge vorzugeben. Keras unterst√ºtzt das, indem wir die 2. Dimension des Input-`shapes` einfach leer lassen.\n",
        "\n",
        "Der shape im Input sollte also so aussehen: `shape=(None,)`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zTZAFKOH_Wt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Input\n",
        "encoder_inputs = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCGmt1MvaP1w",
        "colab_type": "text"
      },
      "source": [
        "Erstellt euch nun einen Embedding Layer f√ºr das englische Vokabular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Y7vO2VcxSJX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Embedding\n",
        "encoder_embedding_layer = #TODO\n",
        "encoder_embeddings = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_cExq21H_Wx",
        "colab_type": "text"
      },
      "source": [
        "Als n√§chstes definiert einen LSTM-Layer, der 256 hidden units hat. Die Ausgabe des Layers soll sp√§ter an den Decoder 'gef√ºttert' werden. Seht in der Dokumentation nach, wie man auf den Thought-Vektor zugreifen kann (https://keras.io/layers/recurrent/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HzhcXZYZH_Wz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM \n",
        "encoder_lstm_layer = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ua0BDZH_W5",
        "colab_type": "text"
      },
      "source": [
        "Verbindet nun den `encoder_embeddings`-Layer mit dem `encoder_lstm_layer`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZy0xucnH_W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_, encoder_state_h, encoder_state_c = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVW2qYucH_W-",
        "colab_type": "text"
      },
      "source": [
        "Der Encoder ist somit fertig implementiert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-VVdFkOH_W_",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Decoder\n",
        "\n",
        "Auch der Decoder f√§ngt mit einem Input-Layer an. Definiert einen Input-Layer analog zum Encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7zT2XsIH_XA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_inputs = # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aegNMq6Xa5f9",
        "colab_type": "text"
      },
      "source": [
        "Erstellt Euch nun einen Embedding Layer f√ºr das deutsche Vokabular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIKapy9NxqW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_embedding_layer = # TODO\n",
        "decoder_embeddings = # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_Za5d5LH_XG",
        "colab_type": "text"
      },
      "source": [
        "Wie auch im Encoder, ist das Herzst√ºck ein LSTM-Layer. Definiert einen LSTM-Layer mit 256 hidden units. Seht in der Dokumentation nach, wie man neben den Thought-Vektoren, auch die komplette Ausgabe-Sequence zur√ºckbekommt."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KChWeaxEH_XG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_lstm_layer = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwMfKLT5H_XL",
        "colab_type": "text"
      },
      "source": [
        "Verbindet nun den `decoder_embeddings`-Layer mit dem `decoder_lstm_layer`. An welcher der drei Ausgaben sind wir hier interessiert? Au√üerdem m√ºssen wir hier, den Thought-Vektor des Encoders mit √ºbergeben, dies geschieht √ºber den `initial_state`-Parameter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiP_8wm_bIVM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_output_states,_,_= # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmF2JJf1H_XQ",
        "colab_type": "text"
      },
      "source": [
        "Den Abschluss unseres Modells bildet ein Dense-Layer, der gleich viele `hidden_units` wie der Embedding-Layer des Decoders hat, da er wieder auf das deutsche Vokabular abbildet. Als Aktivierungsfunktion nehmen wir die Softmax-Funktion."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eO9oBT8ibbym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import Dense\n",
        "\n",
        "decoder_dense_layer = # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmjD75gmH_XU",
        "colab_type": "text"
      },
      "source": [
        "Verbindet diesen Layer mit dem bisherigen Decoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvaKEd43H_XV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_outputs= # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEpvioomH_XY",
        "colab_type": "text"
      },
      "source": [
        "### 2.3 Encoder-Decoder\n",
        "\n",
        "Wir haben nun alle Elemente f√ºr ein Sequence-To-Sequence-Modell zusammen. Im letzten Schritt bauen wir das ganze Modell zusammen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOmLGEMhH_XZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import Model\n",
        "\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs=[decoder_outputs])\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"acc\"])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiaAmnQzb3da",
        "colab_type": "text"
      },
      "source": [
        "Mit der folgender Code-Zelle k√∂nnt ihr euch die Modelstruktur genauer anzeigen lassen:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMMizSs0VgAs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import plot_model\n",
        "plot_model(model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ9e8YpbcD5R",
        "colab_type": "text"
      },
      "source": [
        "Nun k√∂nnen wir unser Encoder-Decoder Modell trainieren. Definiert wie gewohnt eure Callbacks (bspw. `EarlyStopping` und `ModelCheckpoint`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1UdBOSx0f4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "early_stopping= # TODO\n",
        "model_checkpoint = # TODO\n",
        "callbacks = [model_checkpoint, early_stopping]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLCItvrYH_Xc",
        "colab_type": "text"
      },
      "source": [
        "Voil√°, nutzt nun `model.fit_generator()` um das Modell zu trainieren."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwivmDcQH_Xc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(\n",
        "    generator=generator_train, \n",
        "    validation_data=generator_val, \n",
        "    callbacks=callbacks, \n",
        "    epochs=epochs\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XxbGf_Ac45Z",
        "colab_type": "text"
      },
      "source": [
        "## 3. √úbersetzung\n",
        "Jetzt kommen wir zum spannenden Teil der Aufgabe. Aktuell ist unser Modell f√ºr das Training noch so aufgebaut, dass der deutsche Satz als Eingabe f√ºr den Decoder ben√∂tigt wird.  In diesem Schritt bauen wir unser Model so um, dass es nur mit dem englischen Satz den deutschen Satz generieren (ak√° √ºbersetzen) kann."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YqRWFJNH_X3",
        "colab_type": "text"
      },
      "source": [
        "Zuerst ist allerdings ein kleiner Zwischenschritt vonn√∂ten. Damit wir aus unseren Indizes wieder Texte bekommen, m√ºssen wir noch das Dictionary `reverse_german_token_index` bef√ºllen, das die \"Umkehrung\" von `german_token_index` ist. Bef√ºllt f√ºr Testzwecke analog auch `reverse_english_token_index`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjec-0kcH_X4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reverse_german_token_index = # TODO\n",
        "reverse_english_token_index = # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylBMavl_H_Xg",
        "colab_type": "text"
      },
      "source": [
        "### 3.1 Inferenzmodell\n",
        "\n",
        "Definiert ein `Model`, das einen Eingabetext enkodiert. Als Eingabe hat das `Model` den bisherigen `encoder_input`-Layer und die Thought-Vektoren des Encoders als Output. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kK8GYU8fH_Xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model = Model() # TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "timOu8lmN-w5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1-jn6RrH_Xm",
        "colab_type": "text"
      },
      "source": [
        "Etwas komplizierter ist das Decoder-Modell. Als Input nimmt es zum einen die Thought-Vektoren des Encoders, zum anderen den `decoder_input`-Layer von oben.\n",
        "F√ºr den Docoder haben wir schon mal etwas vorgeben."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfL2GyIeH_Xn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs=[decoder_state_input_h, decoder_state_input_c]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I36PjQceaU2",
        "colab_type": "text"
      },
      "source": [
        "Neben den beiden Thought-Vektoren des Encoders dient auch die dekodierte Sequenz als Eingabe. Deshalb m√ºssen wir den Embedding-Layer des Decoders als Input √ºbergeben. Definiert erneut einen Input-Layer und verkn√ºpft diesen mit dem trainierten Embedding-Layer des Decoders. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS64H6yQqFSY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_inputs_2 = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P18w22wRNotD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_embedding_layer_2=#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJI5w1n_H_Xq",
        "colab_type": "text"
      },
      "source": [
        "Setzt die beiden Input-Layer `decoder_states_inputs` als `initial_state` und den `decoder_embedding_layer_2`-Layer als Input in den `decoder_lstm_layer` von oben ein."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DuqkHgQH_Xq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_outputs_2, decoder_state_h, decoder_state_c = #TODO\n",
        "# Speichert die docder_states separat ab\n",
        "decoder_states = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkkNW_IOH_Xv",
        "colab_type": "text"
      },
      "source": [
        "Setzt nun den `decoder_outputs_2`-Tensor in den `decoder_dense_layer` von oben ein."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7N6VT3SH_Xw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_final_outputs = #TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIAPu6K1H_Xz",
        "colab_type": "text"
      },
      "source": [
        "Nun k√∂nnen wir ein `Model` zusammenbauen, das uns als Decoder dient. Input ist hier der `decoder_inputs_2`-Layer, zusammen mit den beiden Tensoren die wir in `decoder_states_inputs` gespeichert haben. Als Output dient uns der `decoder_final_outputs`-Tensor und die beiden Decoder Output-Tensoren, die sich in `decoder_final_output` befinden. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0BRHbNpH_Xz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoder_model = Model(\n",
        "    [decoder_inputs_2] + decoder_states_inputs,\n",
        "    [decoder_final_outputs] + decoder_states\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36-mqZy3O88s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_model(decoder_model,show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeVRGnyQH_X6",
        "colab_type": "text"
      },
      "source": [
        "Nun k√∂nnen wir eine Funktion `translate` implementieren, die als Eingabe englische Vokabularindizes nimmt. Wir haben dies schon mal vorbereitet. Schaut Euch trotzdem die einzelnen Schritte in der Funktion an, um zu verstehen wie eine √úbersetzung erzeugt wird.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdvdVRQMH_X7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0] = german_token_index['üè≥Ô∏è']# Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    decoded_sentence = ''\n",
        "    while True:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)# Sample a token\n",
        "      \n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_german_token_index[sampled_token_index]\n",
        "        \n",
        "        # or find stop character.\n",
        "        if (sampled_token == 'üè¥' or\n",
        "           len(decoded_sentence) > 52):\n",
        "            break\n",
        "        decoded_sentence += ' '+sampled_token# Exit condition: either hit max length\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index# Update states\n",
        "        states_value = [h, c]\n",
        "        \n",
        "    return decoded_sentence\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hcQupy_oJV4",
        "colab_type": "text"
      },
      "source": [
        "Nun k√∂nnen wir testen, wie gut unser √úbersetzer funktioniert. Am besten ihr testet das Ganze mit ein paar S√§tzen aus dem Validation Set. \n",
        "Wichtig: die Eingabe muss im Batch erfolgen!\n",
        "Nutzt im Zweifel `np.expand_dims()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMyTFMK0nHg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "english_text=#TODO\n",
        "german_text=translate(english_text)\n",
        "print(german_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ib7ZnglAootI",
        "colab_type": "text"
      },
      "source": [
        "## 4. Optionale Zusatzaufgabe\n",
        "Ihr k√∂nnt in euren Embedding-Layern noch vortrainierte Word Embeddings nutzen. Dies sollte bessere Ergebnisse liefern und die Generalisierungsf√§higkeiten des Models verbessern."
      ]
    }
  ]
}