{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nG90pwjB-cIO"
   },
   "source": [
    "# HSKA AI-Lab RL: Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qCTFrfsv-g5V"
   },
   "source": [
    "## Mount Google Drive as folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ODLMGwrk-gIJ"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "%cd /content/drive/MyDrive/ai-lab rl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3ghLMfm-cIf"
   },
   "source": [
    "Es soll ein DQN Agent trainiert werden, der ein [Atari 2600](https://www.gymlibrary.dev/environments/atari/complete_list/) Spiel spielen kann.\n",
    "Der Ansatz ist frei – ihr könnt euch an Aufgabe 4 orientieren oder die Methode auf eure Art implementieren.\n",
    "\n",
    "### \"Quiz\"\n",
    "\n",
    "- Wann ist der Agent gut genug? Was ist ein gutes Erfolgskriterium?\n",
    "- Was für eine Architektur soll das Q-Network haben?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBDgFNq5-cIf"
   },
   "source": [
    "### It's dangerous to go alone! Take this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bD9bTND8-cIp"
   },
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install gym[atari]==0.12.5\n",
    "%pip install pyglet==1.3.2\n",
    "\n",
    "import gym\n",
    "\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Tuple\n",
    "import time\n",
    "from datetime import datetime\n",
    "from contextlib import suppress\n",
    "from abc import abstractmethod\n",
    "import os, json\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Lambda, multiply, Input\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.compat.v1.losses import huber_loss\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "from loggers import TensorBoardLogger, tf_summary_image\n",
    "\n",
    "%pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from plot_utils import plot_statistics\n",
    "from abstract_agent import AbstractAgent\n",
    "from atari_helpers import LazyFrames, wrap_deepmind, make_atari\n",
    "\n",
    "!apt-get install -y xvfb python-opengl\n",
    "!python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "    from IPython.display import SVG\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D--k_Yo4-cIw"
   },
   "outputs": [],
   "source": [
    "# familiar interface:\n",
    "env = make_atari('CHOOSE_ATARI_GAME')\n",
    "env = wrap_deepmind(env, frame_stack=True)\n",
    "\n",
    "# or vanilla open ai gym:\n",
    "# env = gym.make('EnduroNoFrameskip-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Hier findet ihr ein paar Inspirationen, wo ihr das doch recht aufwendige Training laufen lassen könnt:\n",
    "\n",
    "- [Google Colab](https://colab.research.google.com/)\n",
    "- [Kaggle](https://www.kaggle.com/docs/tpu)\n",
    "- [Gradient Paperspace](https://www.paperspace.com/gradient)\n",
    "- KI-Rechner der Hochschule\n",
    "\n",
    "Unabhängig von eurer Trainingsumgebung: Macht euch Gedanken über Checkpointing.\n",
    "Als Startpunkt geben wir euch einen erweiterten `DQNAgenten` aus Aufgabe 4 mit, der euch ein rudimentäres Checkpointing ermöglicht:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DQNAgent(AbstractAgent):\n",
    "    __slots__ = [\n",
    "        \"action_size\",\n",
    "        \"state_size\",\n",
    "        \"gamma\",\n",
    "        \"epsilon\",\n",
    "        \"epsilon_decay\",\n",
    "        \"epsilon_min\",\n",
    "        \"alpha\",\n",
    "        \"batch_size\",\n",
    "        \"memory_size\",\n",
    "        \"start_replay_step\",\n",
    "        \"target_model_update_interval\",\n",
    "        \"train_freq\",\n",
    "    ]\n",
    "\n",
    "    def __init__(self,\n",
    "                 action_size: int,\n",
    "                 state_size: int,\n",
    "                 gamma: float,\n",
    "                 epsilon: float,\n",
    "                 epsilon_decay: float,\n",
    "                 epsilon_min: float,\n",
    "                 alpha: float,\n",
    "                 batch_size: int,\n",
    "                 memory_size: int,\n",
    "                 start_replay_step: int,\n",
    "                 target_model_update_interval: int,\n",
    "                 train_freq: int,\n",
    "                 ):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.memory_size = memory_size\n",
    "        self.memory = deque(maxlen=self.memory_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.step = 0\n",
    "        self.start_replay_step = start_replay_step\n",
    "\n",
    "        self.target_model_update_interval = target_model_update_interval\n",
    "\n",
    "        self.train_freq = train_freq\n",
    "\n",
    "        assert self.start_replay_step >= self.batch_size, \"The number of steps to start replay must be at least as large as the batch size\"\n",
    "\n",
    "        self.action_mask = np.ones((1, self.action_size))\n",
    "        self.action_mask_batch = np.ones((self.batch_size, self.action_size))\n",
    "\n",
    "        self.tf_config_intra_threads = 8\n",
    "        self.tf_config_inter_threads = 4\n",
    "        self.tf_config_soft_placement = True\n",
    "        self.tf_config_allow_growth = True\n",
    "\n",
    "        config = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=self.tf_config_intra_threads,\n",
    "                                inter_op_parallelism_threads=self.tf_config_inter_threads,\n",
    "                                allow_soft_placement=self.tf_config_soft_placement\n",
    "                                )\n",
    "\n",
    "        config.gpu_options.allow_growth = self.tf_config_allow_growth\n",
    "        session = tf.compat.v1.Session(config=config)\n",
    "        set_session(session)  # set this TensorFlow session as the default session for Keras\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "\n",
    "    def save(self, target_path: str) -> None:\n",
    "      \"\"\"\n",
    "        Saves the current state of the DQNAgent to some output files.\n",
    "        Together with `load` this serves as a very rudimentary checkpointing.\n",
    "      \"\"\"\n",
    "      agent_dict = {\n",
    "            \"agent_init\": {},\n",
    "            \"agent_params\": {},\n",
    "            \"tf_config\": {}\n",
    "        }\n",
    "\n",
    "      if not os.path.exists(target_path):\n",
    "        os.makedirs(target_path)\n",
    "\n",
    "      for slot in self.__slots__:\n",
    "          agent_dict[\"agent_init\"].update({slot: getattr(self, slot)})\n",
    "\n",
    "      agent_dict[\"agent_init\"].update({\"memory_size\": self.memory.maxlen})\n",
    "\n",
    "      for attr in [\"action_mask\", \"action_mask_batch\"]:\n",
    "          agent_dict[\"agent_params\"].update({attr: getattr(self, attr).tolist()})\n",
    "\n",
    "      agent_dict[\"agent_params\"].update({\"memory\": list(self.memory)})\n",
    "\n",
    "      for tf_config in [\n",
    "          \"tf_config_intra_threads\",\n",
    "          \"tf_config_inter_threads\",\n",
    "          \"tf_config_soft_placement\",\n",
    "          \"tf_config_allow_growth\",\n",
    "      ]:\n",
    "          agent_dict[\"tf_config\"].update({tf_config: getattr(self, tf_config)})\n",
    "\n",
    "      with open(os.path.join(target_path, \"agent.json\"), \"w\") as f:\n",
    "          json.dump(agent_dict, f)\n",
    "\n",
    "      self.model.save_weights(os.path.join(target_path, \"model.h5\"))\n",
    "      self.target_model.save_weights(os.path.join(target_path, \"target_model.h5\"))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> \"DQNAgent\":\n",
    "      \"\"\"\n",
    "        Loads the serialized state of a DQNAgent and returns an instance of it.\n",
    "      \"\"\"\n",
    "\n",
    "      with open(os.path.join(path, \"agent.json\"), \"r\") as f:\n",
    "          agent_dict = json.load(f)\n",
    "\n",
    "      agent = cls(**agent_dict[\"agent_init\"])\n",
    "\n",
    "      agent.action_mask = np.array(agent_dict[\"agent_params\"][\"action_mask\"])\n",
    "      agent.action_mask_batch = np.array(agent_dict[\"agent_params\"][\"action_mask_batch\"])\n",
    "\n",
    "      config = tf.compat.v1.ConfigProto(\n",
    "          intra_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_intra_threads\"],\n",
    "          inter_op_parallelism_threads=agent_dict[\"tf_config\"][\"tf_config_inter_threads\"],\n",
    "          allow_soft_placement=agent_dict[\"tf_config\"][\"tf_config_soft_placement\"])\n",
    "\n",
    "      config.gpu_options.allow_growth = agent_dict[\"tf_config\"][\"tf_config_allow_growth\"]\n",
    "      session = tf.compat.v1.Session(config=config)\n",
    "      set_session(session)\n",
    "\n",
    "      agent.model.load_weights('model.h5')\n",
    "      agent.target_model.load_weights(\"target_model.h5\")\n",
    "\n",
    "      return agent\n",
    "\n",
    "    @abstractmethod\n",
    "    def train(self, experience):\n",
    "      raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def act(self, state):\n",
    "      raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def _build_model(self):\n",
    "      raise NotImplementedError"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Das soll kein fertiges Checkpointing darstellen, bitte verwendet es nur als Ausgangspunkt/Inspiration und passt es nach euren Bedrüfnisssen an.**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Hier findet ihr noch ein paar allgemeine Infos von [Tensorflow dazu](https://www.tensorflow.org/guide/checkpoint)."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4_atari_pong_dqn.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
